{
    "cells": [{
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import re\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "import plotly.express as px\n",
                "from wordcloud import WordCloud\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# --- 1. DOWNLOAD NLTK HELPERS (Only need to run once) ---\n",
                "nltk.download('punkt')\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('punkt_tab')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2. LOAD YOUR DATA ---\n",
                "try:\n",
                "    df = pd.read_csv('../data/raw_tweets.csv')\n",
                "    print(f\"Successfully loaded {len(df)} tweets.\")\n",
                "except FileNotFoundError:\n",
                "    print(\"ERROR: raw_tweets.csv not found.\")\n",
                "    print(\"Please run the '1_data_collection.ipynb' notebook first.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3. CREATE TEXT CLEANING FUNCTION ---\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "# Add custom stopwords relevant to our topic\n",
                "stop_words = set(stopwords.words('english'))\n",
                "custom_stopwords = ['flipkart', 'amazon', 'sale', 'diwali', 'great', 'indian', 'festival', 'big', 'billion', 'days', 'rt']\n",
                "stop_words.update(custom_stopwords)\n",
                "\n",
                "def clean_text(text):\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # Remove URLs\n",
                "    text = re.sub(r'@\\w+|#\\w+', '', text)          # Remove mentions and hashtags\n",
                "    text = text.lower()                           # Convert to lowercase\n",
                "    text = re.sub(r'[^\\w\\s]', '', text)           # Remove punctuation\n",
                "    tokens = nltk.word_tokenize(text)             # Tokenize\n",
                "    \n",
                "    # Remove stopwords and lemmatize\n",
                "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
                "    \n",
                "    return \" \".join(cleaned_tokens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 4. APPLY THE FUNCTION ---\n",
                "df['CleanedText'] = df['Text'].apply(clean_text)\n",
                "\n",
                "print(\"\\nText cleaning complete. Here's a sample:\")\n",
                "print(df[['Text', 'CleanedText']].head())\n",
                "\n",
                "# --- 5. SAVE THE PROCESSED FILE ---\n",
                "output_path = '../data/processed_tweets.csv'\n",
                "df.to_csv(output_path, index=False)\n",
                "print(f\"\\nCleaned data saved to {output_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install VADER for sentiment analysis\n",
                "!pip install vaderSentiment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
                "import numpy as np\n",
                "\n",
                "# Load the cleaned data\n",
                "df = pd.read_csv('../data/processed_tweets.csv')\n",
                "\n",
                "analyzer = SentimentIntensityAnalyzer()\n",
                "\n",
                "# Get sentiment scores\n",
                "df['SentimentScore'] = df['CleanedText'].astype(str).apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
                "\n",
                "# Classify sentiment based on the score\n",
                "conditions = [\n",
                "    (df['SentimentScore'] >= 0.05),\n",
                "    (df['SentimentScore'] <= -0.05),\n",
                "    (df['SentimentScore'] > -0.05) & (df['SentimentScore'] < 0.05)\n",
                "]\n",
                "values = ['Positive', 'Negative', 'Neutral']\n",
                "\n",
                "df['Sentiment'] = np.select(conditions, values, default='Neutral')\n",
                "\n",
                "# Save the *final* processed file with sentiment scores\n",
                "df.to_csv('../data/processed_tweets.csv', index=False)\n",
                "\n",
                "print(\"Sentiment analysis complete and file updated.\")\n",
                "print(df[['CleanedText', 'SentimentScore', 'Sentiment']].head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install nbformat for Plotly\n",
                "!pip install nbformat"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Sentiment Distribution Pie Chart\n",
                "sentiment_counts = df['Sentiment'].value_counts()\n",
                "\n",
                "fig_pie = px.pie(values=sentiment_counts.values, \n",
                "                 names=sentiment_counts.index, \n",
                "                 title=\"Sentiment Distribution of Diwali Sale Tweets\",\n",
                "                 color=sentiment_counts.index,\n",
                "                 color_discrete_map={'Positive':'#34A853', 'Negative':'#EA4335', 'Neutral':'#FBBC05'})\n",
                "\n",
                "fig_pie.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Word Cloud for all tweets\n",
                "all_text = \" \".join(tweet for tweet in df['CleanedText'].astype(str))\n",
                "\n",
                "if all_text.strip(): # Check if the string is not empty\n",
                "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
                "    \n",
                "    plt.figure(figsize=(10, 5))\n",
                "    plt.imshow(wordcloud, interpolation='bilinear')\n",
                "    plt.axis('off')\n",
                "    plt.title('Most Common Words in Tweets')\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No words found to generate a word cloud.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}